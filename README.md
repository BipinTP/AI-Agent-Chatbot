# ğŸ“š AI-Powered PDF Question-Answering System

An intelligent **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDFs, index their content, and ask questions about them with accurate AI-generated responses.

This end-to-end system uses **FastAPI, LangGraph, Pinecone, Groq LLM, and Streamlit**.

---

## ğŸ·ï¸ Badges

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)
![Streamlit](https://img.shields.io/badge/Streamlit-Frontend-red)
![Pinecone](https://img.shields.io/badge/VectorDB-Pinecone-yellow)
![RAG](https://img.shields.io/badge/AI-RAG%20App-purple)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸš€ Features

### ğŸ“„ Upload PDF Documents  
- Upload one or more PDF files through the Streamlit UI  
- Extracts and processes text from PDFs using `PyPDFLoader`

### ğŸ” Smart Text Chunking + Embeddings  
- Cleans and splits long documents into manageable chunks  
- Uses **HuggingFace embeddings** to convert text into vectors  

### ğŸ—‚ï¸ Pinecone Vector Database  
- Stores embedded chunks in **Pinecone**  
- Enables fast **similarity search** for relevant context  

### ğŸ¤– RAG-Based AI Assistant  
- Combines **retrieved PDF chunks + LLM** for accurate responses  
- Handles follow-up questions using chat history  

### ğŸŒ Optional Web Search (Tavily)  
- Uses **Tavily API** for real-time web information  
- Falls back to web search when PDF context is insufficient  

### ğŸ” LangGraph Agent Workflow  
- Built with **LangGraph** to orchestrate the workflow  
- Routes between:
  - RAG over PDFs  
  - Web Search  
  - Direct LLM Answering  

### ğŸ’¬ Interactive Chat UI (Streamlit)  
- Simple, clean chat interface  
- Shows **chat history** and **workflow trace** (what the agent did step-by-step)

---

## ğŸ§  Tech Stack

### Backend

- âš™ï¸ **FastAPI** â€“ REST API backend  
- ğŸ§© **LangGraph** â€“ Agent workflow & routing logic  
- ğŸ§  **Groq LLMs** â€“ Fast and efficient LLM inference  
- ğŸ—‚ï¸ **Pinecone** â€“ Vector database for document retrieval  
- ğŸ”¡ **HuggingFace Embeddings** â€“ Text embedding models  
- ğŸŒ **Tavily Web Search** â€“ Optional external knowledge  
- ğŸ“„ **PyPDFLoader** â€“ PDF text extraction  

### Frontend

- ğŸ–¥ï¸ **Streamlit** â€“ Web UI for chat and file upload  
- ğŸ”— **Requests** â€“ Communication with FastAPI backend  

---

## ğŸ“¦ Project Structure

```bash
rag_agent_app/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ main.py             # FastAPI entrypoint
â”‚   â”œâ”€â”€ agent.py            # LangGraph agent logic
â”‚   â”œâ”€â”€ config.py           # Settings & environment config
â”‚   â”œâ”€â”€ vectorstore.py      # Pinecone vector store utilities
â”‚
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py              # Streamlit main app
â”‚   â”œâ”€â”€ backend_api.py      # API client for FastAPI
â”‚   â”œâ”€â”€ session_manager.py  # Session & chat history handling
â”‚   â”œâ”€â”€ ui_components.py    # Reusable Streamlit UI components
â”‚
â”‚â”€â”€ .env                    # Environment variables (not committed)
â”‚â”€â”€ README.md               # Project documentation

ğŸ› ï¸ Installation & Setup

1ï¸âƒ£ Clone the repo
git clone https://github.com/your-username/ai-agent-chatbot
cd ai-agent-chatbot

2ï¸âƒ£ Create virtual environment
python -m venv venv
source venv/bin/activate      # Linux/Mac
venv\Scripts\activate         # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add your API keys to .env
PINECONE_API_KEY=your_key
GROQ_API_KEY=your_key
TAVILY_API_KEY=your_key

5ï¸âƒ£ Run backend (FastAPI)
cd backend
uvicorn main:app --reload

6ï¸âƒ£ Run frontend (Streamlit)
cd frontend
streamlit run app.py
